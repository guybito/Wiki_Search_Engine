{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1SdZov-pWcm"
      },
      "source": [
        "### **Inverted Index :**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bVDOlfXphhc"
      },
      "source": [
        "## **Imports and setup :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGf6VZlupVt2",
        "outputId": "d4b36b34-0750-4a7d-d06c-49462b902698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME        PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\n",
            "ir-cluster  GCE       4                                             RUNNING  us-central1-a\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk==3.7 in /opt/conda/miniconda3/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: click in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk==3.7) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk==3.7) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk==3.7) (2022.8.17)\n",
            "Requirement already satisfied: tqdm in /opt/conda/miniconda3/lib/python3.10/site-packages (from nltk==3.7) (4.66.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters list --region us-central1\n",
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes\n",
        "!pip install nltk==3.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYDmuqbpprON",
        "outputId": "466601c5-06ca-4939-f3fe-920211bd3a47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "import hashlib\n",
        "import builtins\n",
        "\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgTlfP37p0TV",
        "outputId": "6f7a32d0-5fed-43fc-e867-e3373e29cc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Mar  6 07:30 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyT4w0Q7p11a"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHas6lRjp3sB",
        "outputId": "841daa48-f970-42b6-d020-3a4c2a4ee478"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - hive</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ir-cluster-m.c.my-projec-411017.internal:38153\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>yarn</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySparkShell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7ae7b19810>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzd_nioYqESE"
      },
      "outputs": [],
      "source": [
        "# Define the bucket name and set up paths for data retrieval\n",
        "bucket_name = '318940913'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if not b.name.startswith('Postings_title/') and not b.name.startswith('Postings_body/') and not b.name.startswith('Dict_folder/') and b.name != 'graphframes.sh':\n",
        "        paths.append(full_path+b.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h0b-lI7qwd7"
      },
      "source": [
        "## **Inverted Index Building :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOJ52wt9qF67",
        "outputId": "1a106130-685f-4bc1-b955-b6c1b95645b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Read Parquet file from the specified paths\n",
        "parquetFile = spark.read.parquet(*paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WP-iYZOCqLkt",
        "outputId": "a8a65a28-c3f6-4220-f8d0-b0fc495dc424"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "# Check if the file inverted_index_gcp.py exists in the home directory\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhgW66JOqQGm"
      },
      "outputs": [],
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNramvkaqaCI"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfA_xjnFqaqy"
      },
      "outputs": [],
      "source": [
        "# Extract text, title, anchor and document ID pairs from the Parquet file\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
        "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
        "doc_anchor_pairs = parquetFile.select(\"anchor_text\").rdd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tDZu9bUrMlW"
      },
      "source": [
        "## **Helper Functions :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExWUekHBrRYO"
      },
      "outputs": [],
      "source": [
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "\n",
        "def tokenize(query):\n",
        "  # filtered_query = filter_the_text(query)\n",
        "  stemmer = PorterStemmer()\n",
        "  return [stemmer.stem(token.group()) for token in RE_WORD.finditer(query.lower()) if token.group() not in all_stopwords]\n",
        "\n",
        "def tokenize_without_stem(query):\n",
        "    \"\"\"Tokenize the input query without stemming.\n",
        "    Args:\n",
        "    - query (str): The input query.\n",
        "    Returns:\n",
        "    - list of str: A list of tokens after tokenization without stemming.\n",
        "    \"\"\"\n",
        "    return [token.group() for token in RE_WORD.finditer(query.lower()) if token.group() not inÂ all_stopwords]\n",
        "\n",
        "\n",
        "\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "  \"\"\"\n",
        "    Map a token to a bucket ID for partitioning postings.\n",
        "    Parameters:\n",
        "        token (str): The input token.\n",
        "    Returns:\n",
        "        int: The bucket ID for the token.\n",
        "  \"\"\"\n",
        "  return int(_hash(token),16) % NUM_BUCKETS\n",
        "\n",
        "def anchor_to_text(item):\n",
        "  # Merge all the references to one list\n",
        "    lst = []\n",
        "    for i in item[0]:\n",
        "        lst.append((i[0],i[1]))\n",
        "    return lst\n",
        "\n",
        "def word_count(text, id):\n",
        "  \"\"\"\n",
        "    Count the occurrences of words in the given text.\n",
        "    Parameters:\n",
        "        text (str): The input text.\n",
        "        id (str): The ID associated with the text.\n",
        "    Returns:\n",
        "        list: A list of tuples containing (word, (document_id, term_frequency)).\n",
        "  \"\"\"\n",
        "  word_counts_map = Counter(tokenize(text))\n",
        "  # Filter out stopwords and create the list of tuples\n",
        "  result = [(token, (id, word_counts_map[token])) for token in word_counts_map]\n",
        "  return result\n",
        "\n",
        "def word_count_without_stem(text, id):\n",
        "  \"\"\"\n",
        "    Count the occurrences of words in the given text.\n",
        "    Parameters:\n",
        "        text (str): The input text.\n",
        "        id (str): The ID associated with the text.\n",
        "    Returns:\n",
        "        list: A list of tuples containing (word, (document_id, term_frequency)).\n",
        "  \"\"\"\n",
        "  word_counts_map = Counter(tokenize_without_stem(text))\n",
        "  # Filter out stopwords and create the list of tuples\n",
        "  result = [(token, (id, word_counts_map[token])) for token in word_counts_map]\n",
        "  return result\n",
        "\n",
        "def word_count_ngrams(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    tokens = [stemmer.stem(t) for t  in tokens if t not in all_stopwords]\n",
        "    tokens = ngrams(tokens,2)\n",
        "    fin_tokens=[]\n",
        "    for tup in tokens:\n",
        "        fin_tokens.append(tup[0]+\" \"+tup[1])\n",
        "\n",
        "    lst = []\n",
        "    c = Counter(fin_tokens)\n",
        "\n",
        "    for t in c:\n",
        "        lst.append((t, (id, c[t])))\n",
        "\n",
        "    return lst\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "  \"\"\"\n",
        "    Reduce unsorted posting lists by sorting them.\n",
        "    Parameters:\n",
        "        unsorted_pl (list): Unsorted posting lists.\n",
        "    Returns:\n",
        "        list: Sorted posting lists.\n",
        "  \"\"\"\n",
        "  return sorted(unsorted_pl)\n",
        "\n",
        "def calculate_df(postings):\n",
        "  \"\"\"\n",
        "    Calculate the document frequency (DF) for each token in the postings.\n",
        "    Parameters:\n",
        "        postings (RDD): RDD containing posting lists for tokens.\n",
        "    Returns:\n",
        "        RDD: RDD containing tuples of token and its document frequency.\n",
        "  \"\"\"\n",
        "  token_df = postings.map(lambda token_tuple: (token_tuple[0], len(token_tuple[1])))\n",
        "  return token_df\n",
        "\n",
        "def partition_postings_and_write(postings,folder_name,bucket_name):\n",
        "  \"\"\"\n",
        "    Partition postings and write them to storage.\n",
        "    Parameters:\n",
        "        postings (RDD): RDD containing posting lists.\n",
        "        bucket_name (str): Name of the storage bucket.\n",
        "        folder_name (str): Name of the folder within the storage bucket.\n",
        "    Returns:\n",
        "        RDD: Partitioned postings with bucket IDs.\n",
        "  \"\"\"\n",
        "  partition_postings = postings.map(lambda token_tuple: (token2bucket_id(token_tuple[0]), token_tuple))\n",
        "  return partition_postings.groupByKey().map(lambda token_tuple: InvertedIndex.write_a_posting_list(token_tuple,folder_name, bucket_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwFJgAGOMPZk"
      },
      "source": [
        "## **Inverted Index for the Title**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExlMN1Y0MPZl"
      },
      "outputs": [],
      "source": [
        "def Create_inverted_index_title(data):\n",
        "  \"\"\"\n",
        "    Create an inverted index for document retrieval based on the document titles.\n",
        "\n",
        "    Parameters:\n",
        "        data (RDD): RDD containing document data, where each element is a tuple (document_id, title_text).\n",
        "        index_name (str): Name of the index file to be created.\n",
        "        bucket_name (str): Name of the storage bucket where the index will be stored.\n",
        "        folder_name (str): Name of the folder within the storage bucket where index files will be stored.\n",
        "\n",
        "    Returns:\n",
        "        InvertedIndex: An inverted index object containing the constructed index.\n",
        "  \"\"\"\n",
        "  #count words\n",
        "  word_counts = data.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "  #posting list\n",
        "  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  #df calc\n",
        "  w2df_t = calculate_df(postings)\n",
        "  w2df_dict_t = w2df_t.collectAsMap()\n",
        "  # write the index\n",
        "  _ = partition_postings_and_write(postings,\"Postings_title\",bucket_name).collect()\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix=\"Postings_title\"):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "      for k, v in posting_locs.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "  inverted_index_title = InvertedIndex()\n",
        "  inverted_index_title.posting_locs = super_posting_locs\n",
        "  inverted_index_title.df = w2df_dict_t\n",
        "  inverted_index_title.write_index('.', \"Title_Inverted_Index\")\n",
        "  return inverted_index_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b31iVkjwMPZl"
      },
      "outputs": [],
      "source": [
        "# Create an inverted index for document titles\n",
        "inverted_index_title=Create_inverted_index_title(doc_title_pairs)\n",
        "print(\"Building the index for title completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhlC02XQMPZm"
      },
      "outputs": [],
      "source": [
        "## upload to gs\n",
        "# Define the source path of the index file\n",
        "index_src = \"Title_Inverted_Index.pkl\"\n",
        "# Define the destination path where the index file will be copied\n",
        "index_dst = f'gs://{bucket_name}/Postings_title/{index_src}'\n",
        "# Use the gsutil command-line tool to copy the index file from the source path to the destination path\n",
        "!gsutil cp $index_src $index_dst\n",
        "# Use the gsutil command-line tool to list detailed information about the copied index file\n",
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig3C7lbnL4iU"
      },
      "source": [
        "## **Inverted Index for the Title Without Stemming**:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Create_inverted_index_title_without_stem(data):\n",
        "  \"\"\"\n",
        "    Create an inverted index for document retrieval based on the document titles.\n",
        "\n",
        "    Parameters:\n",
        "        data (RDD): RDD containing document data, where each element is a tuple (document_id, title_text).\n",
        "        index_name (str): Name of the index file to be created.\n",
        "        bucket_name (str): Name of the storage bucket where the index will be stored.\n",
        "        folder_name (str): Name of the folder within the storage bucket where index files will be stored.\n",
        "\n",
        "    Returns:\n",
        "        InvertedIndex: An inverted index object containing the constructed index.\n",
        "  \"\"\"\n",
        "  #count words\n",
        "  word_counts = data.flatMap(lambda x: word_count_without_stem(x[0], x[1]))\n",
        "  #posting list\n",
        "  postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  #df calc\n",
        "  w2df_t = calculate_df(postings)\n",
        "  w2df_dict_t = w2df_t.collectAsMap()\n",
        "  # write the index\n",
        "  _ = partition_postings_and_write(postings,\"Postings_title_without_stem\",bucket_name).collect()\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix=\"Postings_title_without_stem\"):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "      for k, v in posting_locs.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "  inverted_index_title = InvertedIndex()\n",
        "  inverted_index_title.posting_locs = super_posting_locs\n",
        "  inverted_index_title.df = w2df_dict_t\n",
        "  inverted_index_title.write_index('.', \"Title_Without_Stem_Inverted_Index\")\n",
        "  return inverted_index_title"
      ],
      "metadata": {
        "id": "XFefefczMueW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZzkFtTONJ8I"
      },
      "outputs": [],
      "source": [
        "# Create an inverted index for document titles\n",
        "inverted_index_title_without_stem=Create_inverted_index_title_without_ste(doc_title_pairs)\n",
        "print(\"Building the index for title completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNknHpGzNMvx"
      },
      "outputs": [],
      "source": [
        "## upload to gs\n",
        "# Define the source path of the index file\n",
        "index_src = \"Title_Without_Stem_Inverted_Index.pkl\"\n",
        "# Define the destination path where the index file will be copied\n",
        "index_dst = f'gs://{bucket_name}/Postings_title_without_stem/{index_src}'\n",
        "# Use the gsutil command-line tool to copy the index file from the source path to the destination path\n",
        "!gsutil cp $index_src $index_dst\n",
        "# Use the gsutil command-line tool to list detailed information about the copied index file\n",
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOe3DnMNuKHD"
      },
      "source": [
        "## **Inverted Index for the Body**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wfk0jENuNq6"
      },
      "outputs": [],
      "source": [
        "def Create_inverted_index_body(data):\n",
        "  \"\"\"\n",
        "    Create an inverted index using the BM25 algorithm for document retrieval.\n",
        "\n",
        "    Parameters:\n",
        "        data (RDD): RDD containing document data.\n",
        "        index_name (str): Name of the index file.\n",
        "        bucket_name (str): Name of the storage bucket.\n",
        "        folder_name (str): Name of the folder in the storage bucket.\n",
        "\n",
        "    Returns:\n",
        "        InvertedIndex: An inverted index object containing the constructed index.\n",
        "  \"\"\"\n",
        "  #count words\n",
        "  word_counts = data.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "  #posting list\n",
        "  postings_text = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  postings_filtered = postings_text.filter(lambda x: len(x[1])>50)\n",
        "  #df calc\n",
        "  w2df = calculate_df(postings_filtered)\n",
        "  w2df_dict = w2df.collectAsMap()\n",
        "  # write the index\n",
        "  _ = partition_postings_and_write(postings_filtered,\"Postings_body\",bucket_name).collect()\n",
        "  # posting_locs_list_text = partition_postings_and_write(postings_filtered,\"BodyBins\").collect()\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix=\"Postings_body\"):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "      for k, v in posting_locs.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "  # update fields\n",
        "  inverted_index_body = InvertedIndex()\n",
        "  inverted_index_body.posting_locs = super_posting_locs\n",
        "  inverted_index_body.df = w2df_dict\n",
        "  inverted_index_body.write_index('.', \"Body_Inverted_Index\")\n",
        "  return inverted_index_body"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNauENR4252a"
      },
      "outputs": [],
      "source": [
        "# Create an inverted index for document titles using the BM25 algorithm\n",
        "inverted_index_body=Create_inverted_index_body(doc_text_pairs)\n",
        "print(\"Building the index for body completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN7i9QBw3ecd"
      },
      "outputs": [],
      "source": [
        "## upload to gs\n",
        "# Define the source path of the index file\n",
        "index_src = \"Body_Inverted_Index.pkl\"\n",
        "# Define the destination path where the index file will be copied\n",
        "index_dst = f'gs://{bucket_name}/Postings_body/{index_src}'\n",
        "# Use the gsutil command-line tool to copy the index file from the source path to the destination path\n",
        "!gsutil cp $index_src $index_dst\n",
        "# Use the gsutil command-line tool to list detailed information about the copied index file\n",
        "!gsutil ls -lh $index_dst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLQiPKciNjY7"
      },
      "source": [
        "# **Inverted Index for the Body With N-Grams**:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Create_inverted_index_body_ngrams(data):\n",
        "  #count words\n",
        "  word_counts = data.flatMap(lambda x: word_count_ngrams(x[0], x[1]))\n",
        "  postings_text = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  postings_filtered = postings_text.filter(lambda x: len(x[1])>50)\n",
        "  #df calc\n",
        "  w2df = calculate_df(postings_filtered)\n",
        "  w2df_dict = w2df.collectAsMap()\n",
        "  # write the index\n",
        "  _ = partition_postings_and_write(postings_filtered,\"Postings_body_Ngrams\",bucket_name).collect()\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix=\"Postings_body_Ngrams\"):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "      for k, v in posting_locs.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "  # update fields\n",
        "  inverted_index_body_ngrams = InvertedIndex()\n",
        "  inverted_index_body_ngrams.posting_locs = super_posting_locs\n",
        "  inverted_index_body_ngrams.df = w2df_dict\n",
        "  inverted_index_body_ngrams.write_index('.', \"Body_Inverted_Index_Ngrams\")\n",
        "  return inverted_index_body_ngrams"
      ],
      "metadata": {
        "id": "n5OIZzQPNwSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an inverted index for body with ngrams\n",
        "inverted_index_body_ngrams=Create_inverted_index_body_ngrams(doc_text_pairs)\n",
        "print(\"Building the index for body with ngrams completed successfully\")"
      ],
      "metadata": {
        "id": "gWwKbs7uNzKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## upload to gs\n",
        "# Define the source path of the index file\n",
        "index_src = \"Body_Inverted_Index_Ngrams.pkl\"\n",
        "# Define the destination path where the index file will be copied\n",
        "index_dst = f'gs://{bucket_name}/Postings_body_Ngrams/{index_src}'\n",
        "# Use the gsutil command-line tool to copy the index file from the source path to the destination path\n",
        "!gsutil cp $index_src $index_dst\n",
        "# Use the gsutil command-line tool to list detailed information about the copied index file\n",
        "!gsutil ls -lh $index_dst"
      ],
      "metadata": {
        "id": "EdzXECRwN1M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inverted Index for the Anchor**:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "J6aE2Rn3MPZp"
      }
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "## BODY\n",
        "def Create_inverted_index_anchor(data):\n",
        "  \"\"\"\n",
        "    Create an inverted index for document retrieval based on the document anchor text.\n",
        "\n",
        "    Parameters:\n",
        "        data (RDD): RDD containing document data, where each element is a tuple (document_id, title_text).\n",
        "        index_name (str): Name of the index file to be created.\n",
        "        bucket_name (str): Name of the storage bucket where the index will be stored.\n",
        "        folder_name (str): Name of the folder within the storage bucket where index files will be stored.\n",
        "\n",
        "    Returns:\n",
        "        InvertedIndex: An inverted index object containing the constructed index.\n",
        "  \"\"\"\n",
        "  #count words\n",
        "  word_counts = data.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "  #posting list\n",
        "  postings_text = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "  postings_filtered = postings_text.filter(lambda x: len(x[1])>50)\n",
        "  #df calc\n",
        "  w2df = calculate_df(postings_filtered)\n",
        "  w2df_dict = w2df.collectAsMap()\n",
        "  # write the index\n",
        "  _ = partition_postings_and_write(postings_filtered,\"Postings_anchor\",bucket_name).collect()\n",
        "  # posting_locs_list_text = partition_postings_and_write(postings_filtered,\"BodyBins\").collect()\n",
        "  super_posting_locs = defaultdict(list)\n",
        "  for blob in client.list_blobs(bucket_name, prefix=\"Postings_anchor\"):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "      continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "      posting_locs = pickle.load(f)\n",
        "      for k, v in posting_locs.items():\n",
        "        super_posting_locs[k].extend(v)\n",
        "  # update fields\n",
        "  inverted_index_anchor = InvertedIndex()\n",
        "  inverted_index_anchor.posting_locs = super_posting_locs\n",
        "  inverted_index_anchor.df = w2df_dict\n",
        "  inverted_index_anchor.write_index('.', \"Anchor_Inverted_Index\")\n",
        "  return inverted_index_anchor"
      ],
      "metadata": {
        "id": "IzV64EHwMPZq"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "# Create an inverted index for Anchor\n",
        "# Transformations to create anchor_text_pairs RDD\n",
        "anchor_text_pairs = (\n",
        "    doc_anchor_pairs\n",
        "    .flatMap(anchor_to_text)  # Convert anchor-doc pairs to list of (anchor, doc) tuples\n",
        "    .distinct()               # Remove duplicate (anchor, doc) tuples\n",
        "    .groupByKey()             # Group tuples by anchor\n",
        "    .mapValues(list)          # Convert values (iterator) to lists\n",
        ")\n",
        "\n",
        "# Further transformation to create anchor_text RDD\n",
        "anchor_text = (anchor_text_pairs.map(lambda x: (\" \".join(x[1]), x[0])))  # Concatenate anchors and map to (text, anchor) tuples\n",
        "inverted_index_anchor=Create_inverted_index_anchor(anchor_text)\n",
        "print(\"Building the index for anchor completed successfully\")"
      ],
      "metadata": {
        "id": "h4T45JmnMPZq"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "## upload to gs\n",
        "# Define the source path of the index file\n",
        "index_src = \"Anchor_Inverted_Index.pkl\"\n",
        "# Define the destination path where the index file will be copied\n",
        "index_dst = f'gs://{bucket_name}/Postings_anchor/{index_src}'\n",
        "# Use the gsutil command-line tool to copy the index file from the source path to the destination path\n",
        "!gsutil cp $index_src $index_dst\n",
        "# Use the gsutil command-line tool to list detailed information about the copied index file\n",
        "!gsutil ls -lh $index_dst"
      ],
      "metadata": {
        "id": "jXOxN-WnMPZr"
      },
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
