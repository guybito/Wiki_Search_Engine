{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Imports and setup :**\n"
      ],
      "metadata": {
        "id": "6GfLYD76AwMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters list --region us-central1\n",
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes\n",
        "!pip install nltk==3.7"
      ],
      "metadata": {
        "id": "iPCUBQ0eBLWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from timeit import timeit\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "from google.cloud import storage\n",
        "import math\n",
        "import hashlib\n",
        "import builtins\n",
        "\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Z452fXTrBM3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /usr/lib/spark/jars/graph*"
      ],
      "metadata": {
        "id": "7YtA9wIkBOwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ],
      "metadata": {
        "id": "gKeIPEy4BRZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "dbSgrJhBBS0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the bucket name and set up paths for data retrieval\n",
        "bucket_name = '318940913'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if not b.name.startswith('Postings_title/') and not b.name.startswith('Dict_folder/') and not b.name.startswith('Postings_body/') and b.name != 'graphframes.sh':\n",
        "        paths.append(full_path+b.name)"
      ],
      "metadata": {
        "id": "bvtm93j_BTZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Parquet file from the specified paths\n",
        "parquetFile = spark.read.parquet(*paths)"
      ],
      "metadata": {
        "id": "nmfVegSnBboN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the file inverted_index_gcp.py exists in the home directory\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ],
      "metadata": {
        "id": "Y6CCB-xIBcjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ],
      "metadata": {
        "id": "HEALkaV2BdxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_gcp import *"
      ],
      "metadata": {
        "id": "l5aHe757BfwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract anchor and document ID pairs from the Parquet file\n",
        "doc_anchor_pairs = parquetFile.select(\"id\",\"anchor_text\").rdd"
      ],
      "metadata": {
        "id": "1x3KFoYKBhOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph(pages):\n",
        "  # Define a function to extract unique page IDs and anchor texts\n",
        "  def get_ids_anchor_text_tuple(row):\n",
        "    lst = []\n",
        "    for r in row:\n",
        "      lst.append(r[0])\n",
        "    return list(set(lst))\n",
        "  # Define a function to generate edges between page IDs\n",
        "  def get_edges(ids):\n",
        "    lst = []\n",
        "    for id in ids[1]:\n",
        "      lst.append((ids[0], id))\n",
        "    return lst\n",
        "  # Define a function to generate vertices from edges\n",
        "  def get_vertices(edges):\n",
        "    lst = []\n",
        "    for i in edges:\n",
        "      lst.append((i,))\n",
        "    return lst\n",
        "  # Map page IDs to their respective anchor text lists\n",
        "  ids = pages.mapValues(get_ids_anchor_text_tuple)\n",
        "  # Generate edges between page IDs\n",
        "  edges = ids.flatMap(get_edges)\n",
        "  # Generate vertices from edges\n",
        "  vertices = edges.flatMap(get_vertices).distinct()\n",
        "  return edges, vertices\n"
      ],
      "metadata": {
        "id": "h2kRkdR7Bi-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate graph from document anchor pairs\n",
        "edges, vertices = generate_graph(doc_anchor_pairs)\n",
        "# Convert RDDs to DataFrames and repartition\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "# Create a GraphFrame from vertices and edges\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "# Run PageRank algorithm on the graph\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr_pandas = pr.toPandas()\n",
        "pr_dict = pr_pandas.set_index('id').to_dict()['pagerank']\n",
        "\n",
        "# Save the dictionary as pickle file\n",
        "x=\"page_rank\"\n",
        "file_name = f\"{x}.pickle\"\n",
        "folder_name = \"Page_Rank\"\n",
        "with open(file_name, \"wb\") as f:\n",
        "    pickle.dump(pr_dict, f)\n",
        "\n",
        "client = storage.Client()\n",
        "bucket = client.bucket(bucket_name)\n",
        "\n",
        "blob_page_rank = bucket.blob(f\"{folder_name}/{file_name}\")\n",
        "blob_page_rank.upload_from_filename(file_name)"
      ],
      "metadata": {
        "id": "LwFRAyR4BxUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}