{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSraCg1NEZX_"
   },
   "outputs": [],
   "source": [
    "## **Imports and setup :**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kp3lAL-8Eayk"
   },
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters list --region us-central1\n",
    "!pip install -q google-cloud-storage==1.43.0\n",
    "!pip install -q graphframes\n",
    "!pip install nltk==3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuZNwKvAEcF4"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import itertools\n",
    "from itertools import islice, count, groupby\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "from timeit import timeit\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import math\n",
    "import hashlib\n",
    "import builtins\n",
    "\n",
    "def _hash(s):\n",
    "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ_iWld3Ed2_"
   },
   "outputs": [],
   "source": [
    "!ls -l /usr/lib/spark/jars/graph*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIvXUNXZEfMT"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf, SparkFiles\n",
    "from pyspark.sql import SQLContext\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKThD049EgQZ"
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGB5SnGSEhmK"
   },
   "outputs": [],
   "source": [
    "# Define the bucket name and set up paths for data retrieval\n",
    "bucket_name = '318940913'\n",
    "full_path = f\"gs://{bucket_name}/\"\n",
    "paths=[]\n",
    "\n",
    "client = storage.Client()\n",
    "blobs = client.list_blobs(bucket_name)\n",
    "for b in blobs:\n",
    "    if not b.name.startswith('Postings_title/') and not b.name.startswith('Postings_body/') and not b.name.startswith('Dict_folder/') and b.name != 'graphframes.sh':\n",
    "        paths.append(full_path+b.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As2a06TyEzz1"
   },
   "outputs": [],
   "source": [
    "# Read Parquet file from the specified paths\n",
    "parquetFile = spark.read.parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9c3zSUf7E00Y"
   },
   "outputs": [],
   "source": [
    "# Check if the file inverted_index_gcp.py exists in the home directory\n",
    "%cd -q /home/dataproc\n",
    "!ls inverted_index_gcp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qfp7I4J5E2FJ"
   },
   "outputs": [],
   "source": [
    "# adding our python module to the cluster\n",
    "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
    "sys.path.insert(0,SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVYhQMlXE4E5"
   },
   "outputs": [],
   "source": [
    "from inverted_index_gcp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-XgoAiOE4jl"
   },
   "outputs": [],
   "source": [
    "# Extract text, title and document ID pairs from the Parquet file\n",
    "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
    "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaDQaapLFKWU"
   },
   "source": [
    "## Helper functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yM_txEQ7FJoX"
   },
   "outputs": [],
   "source": [
    "english_stopwords = frozenset(stopwords.words('english'))\n",
    "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
    "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
    "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
    "                    \"many\", \"however\", \"would\", \"became\"]\n",
    "\n",
    "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
    "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "\n",
    "def tokenize(query):\n",
    "    \"\"\"\n",
    "    Tokenize the input query and remove stopwords.\n",
    "    Parameters:\n",
    "        query (str): The input text to be tokenized.\n",
    "    Returns:\n",
    "        list: List of tokens after tokenization, stemming and stopword removal.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token.group()) for token in RE_WORD.finditer(query.lower()) if token.group() not in all_stopwords]\n",
    "\n",
    "NUM_BUCKETS = 124\n",
    "def token2bucket_id(token):\n",
    "    \"\"\"\n",
    "    Map a token to a bucket ID for partitioning postings.\n",
    "    Parameters:\n",
    "        token (str): The input token.\n",
    "    Returns:\n",
    "        int: The bucket ID for the token.\n",
    "    \"\"\"\n",
    "    return int(_hash(token),16) % NUM_BUCKETS\n",
    "\n",
    "def build_norm_doc(tok_text):\n",
    "    \"\"\"\n",
    "    Build the normalized document vector.\n",
    "    Parameters:\n",
    "        tok_text (list): List of tokens in the document.\n",
    "    Returns:\n",
    "        float: Normalized document vector.\n",
    "    \"\"\"\n",
    "    temp_dict = {}\n",
    "    for term in list(tok_text):\n",
    "        if term not in temp_dict:\n",
    "            temp_dict[term] = 0\n",
    "        temp_dict[term] +=1\n",
    "    sum = 0\n",
    "    for term in temp_dict:\n",
    "        sum += temp_dict[term]**2\n",
    "    if sum == 0:\n",
    "        return sum\n",
    "    return 1/math.sqrt(sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyQjE3ifIGAr"
   },
   "source": [
    "## Create a dictionary that includes (doc_id : doc_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCAZYz--FwEf"
   },
   "outputs": [],
   "source": [
    "# Map document ID to document length and save to a dictionary\n",
    "Docs_len_dict= doc_text_pairs.map(lambda x: (x[1], len(tokenize(x[0])))).collectAsMap()\n",
    "\n",
    "x=\"Docs_len_Body_Dict\"\n",
    "file_name = f\"{x}.pickle\"\n",
    "folder_name = \"Dict_folder\"\n",
    "print(\"file name : \",file_name)\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(Docs_len_dict, f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "blob_Docs_len_dict = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "blob_Docs_len_dict.upload_from_filename(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLu82AyyIOsC"
   },
   "source": [
    "## Create a dictionary that includes (doc_id : title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAq9Qxu8ISZy"
   },
   "outputs": [],
   "source": [
    "# Map document ID to document title and save to a dictionary\n",
    "id_title_dict=dict(doc_title_pairs.collectAsMap())\n",
    "\n",
    "x=\"id_title_dict\"\n",
    "file_name = f\"{x}.pickle\"\n",
    "folder_name = \"Dict_folder\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(id_title_dict, f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "blob_id_title_dict = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "blob_id_title_dict.upload_from_filename(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI-N9zWQLGgI"
   },
   "source": [
    "## Create a document with the average length of the corpus documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN0a80mrJE2-"
   },
   "outputs": [],
   "source": [
    "# Calculate average document length and save to a file\n",
    "total_docs = doc_text_pairs.count()\n",
    "total_len_docs = doc_text_pairs.map(lambda x: len(tokenize(x[0]))).reduce(lambda x, y: x + y)\n",
    "avg_doc_len = total_len_docs / total_docs\n",
    "\n",
    "x=\"docs_avg_len\"\n",
    "file_name = f\"{x}.pickle\"\n",
    "folder_name = \"Dict_folder\"\n",
    "print(\"file name : \",file_name)\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(avg_doc_len, f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "blob_avg_doc_len = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "blob_avg_doc_len.upload_from_filename(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dictionary that includes (doc_id : norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map document ID to document norm and save to a dictionary\n",
    "doc_text_pairs_new = doc_text_pairs.mapValues(tokenize).mapValues(build_norm_doc)\n",
    "norm_dict= doc_text_pairs_new.collectAsMap()\n",
    "\n",
    "x=\"norm_dict\"\n",
    "file_name = f\"{x}.pickle\"\n",
    "folder_name = \"Dict_folder\"\n",
    "\n",
    "with open(file_name, \"wb\") as f:\n",
    "    pickle.dump(norm_dict, f)\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "folder_blob = bucket.blob(folder_name)\n",
    "if not folder_blob.exists():\n",
    "    folder_blob.upload_from_string('')\n",
    "\n",
    "blob_norm_dict = bucket.blob(f\"{folder_name}/{file_name}\")\n",
    "blob_norm_dict.upload_from_filename(file_name)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}